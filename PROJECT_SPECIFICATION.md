# Predictive Maintenance MLOps Platform
## Project Specification (Personal Portfolio Edition)

**Last Updated:** February 7, 2026

---

## üéØ Project Vision

Build a **production-ready** predictive maintenance platform that demonstrates end-to-end MLOps capabilities, leveraging deep learning with GPU acceleration. This project serves as a portfolio piece showcasing real-world ML engineering skills.

**Key Differentiators:**
- GPU-accelerated LSTM with PyTorch (RTX 5060 Mobile, 8GB VRAM)
- Real MLOps pipeline (not just a Jupyter notebook)
- Explainable predictions with SHAP
- Automated retraining with drift detection
- Docker-based deployment ready for cloud

---

## üíª Hardware & Environment

**Development Machine:**
- GPU: NVIDIA RTX 5060 Mobile (8GB VRAM)
- CUDA: 13.1
- Driver: 591.74
- Available VRAM for Training: ~7GB (after OS overhead)

**Performance Targets:**
- LSTM training: 5-10 minutes per epoch
- Batch inference: 100 predictions < 2 seconds
- API latency: < 50ms (p95)

---

## üìã Project Phases (12-Week Timeline)

### **Phase 1: Foundation (Weeks 1-3)**
**Goal:** Working baseline with data pipeline and simple model

**Deliverables:**
- ‚úÖ EDA notebook with dataset understanding
- ‚úÖ Feature engineering pipeline (30-50 features)
- ‚úÖ XGBoost baseline model (target F2 > 0.75)
- ‚úÖ MLflow experiment tracking
- ‚úÖ Basic project structure

**Tech Stack:**
- Python 3.11
- Pandas, NumPy, Scikit-learn
- XGBoost
- MLflow
- Jupyter Lab

### **Phase 2: Deep Learning & API (Weeks 4-6)**
**Goal:** Production API with ensemble model

**Deliverables:**
- ‚úÖ PyTorch LSTM model (GPU-accelerated)
- ‚úÖ Ensemble: XGBoost (0.6) + LSTM (0.4)
- ‚úÖ FastAPI prediction service
- ‚úÖ SHAP explainability
- ‚úÖ Docker Compose setup
- ‚úÖ Test coverage > 80%

**Tech Stack:**
- PyTorch 2.x + CUDA 13.1
- FastAPI + Uvicorn
- Redis (feature store)
- PostgreSQL (data lake)
- Docker + Docker Compose

### **Phase 3: MLOps Pipeline (Weeks 7-9)**
**Goal:** Automated training and deployment

**Deliverables:**
- ‚úÖ Automated training pipeline
- ‚úÖ Model registry (MLflow)
- ‚úÖ Drift detection (Evidently AI)
- ‚úÖ Automated retraining trigger
- ‚úÖ Model versioning and rollback
- ‚úÖ CI/CD with GitHub Actions

**Tech Stack:**
- Prefect (lightweight orchestration)
- Evidently AI (drift detection)
- Great Expectations (data validation)
- GitHub Actions (CI/CD)

### **Phase 4: Monitoring & Polish (Weeks 10-12)**
**Goal:** Production-grade monitoring and documentation

**Deliverables:**
- ‚úÖ Prometheus + Grafana dashboards
- ‚úÖ Alerting (Slack webhook)
- ‚úÖ Load testing (Locust)
- ‚úÖ Complete documentation
- ‚úÖ Demo video (5-8 minutes)
- ‚úÖ Technical blog post

**Tech Stack:**
- Prometheus (metrics)
- Grafana (visualization)
- Locust (load testing)
- MkDocs (documentation)

---

## ü§ñ Machine Learning Design

### **Problem Statement**

**Binary Classification:** Predict equipment failure in next 30 cycles
**Regression:** Estimate Remaining Useful Life (RUL)

**Dataset:** NASA Turbofan Engine Degradation
- Training samples: ~70,000 cycles
- Engines: 100 units
- Sensors: 21 time-series measurements
- Operating conditions: 3 settings

### **Target Metrics**

| Metric | Target | Rationale |
|--------|--------|-----------|
| **F2 Score** | > 0.80 | Prioritize recall (2x weight) - missing failures costly |
| **Precision** | > 0.65 | Acceptable false alarm rate (~35%) |
| **Recall** | > 0.85 | Catch 85%+ of actual failures |
| **AUC-ROC** | > 0.90 | Overall discrimination ability |
| **RMSE (RUL)** | < 20 cycles | RUL prediction accuracy |

**Business Context:**
- False Negative Cost: $50,000 (unplanned downtime)
- False Positive Cost: $2,000 (unnecessary inspection)
- Target: 10:1 cost ratio justifies recall focus

### **Model Architecture**

#### **Model 1: XGBoost (Weight: 0.60)**

```python
XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=5,  # class imbalance
    gpu_id=0,  # GPU acceleration
    tree_method='gpu_hist'
)
```

**Strengths:**
- Handles tabular features excellently
- Fast inference (<2ms per prediction)
- Interpretable feature importance
- Robust to feature scaling

**Training Time:** ~5 minutes
**Expected F2:** 0.76-0.78

#### **Model 2: LSTM (Weight: 0.40)**

```python
class LSTMPredictor(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm1 = nn.LSTM(input_size=21, hidden_size=128, batch_first=True)
        self.dropout1 = nn.Dropout(0.3)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)
        self.dropout2 = nn.Dropout(0.2)
        self.fc1 = nn.Linear(64, 32)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
```

**Input:** Last 50 cycles √ó 21 sensors
**Strengths:**
- Captures temporal dependencies
- Learns degradation patterns
- Benefits from GPU (8GB VRAM sufficient)

**Training Time:** ~8 minutes (10 epochs on GPU)
**Expected F2:** 0.74-0.76

#### **Ensemble Strategy**

**Weighted Average:**
```python
prediction = 0.6 * xgb_proba + 0.4 * lstm_proba
```

**Calibration:** Platt scaling on validation set
**Expected Ensemble F2:** 0.82-0.85

**Why this split?**
- XGBoost excels at cross-sectional patterns
- LSTM captures temporal degradation
- XGBoost is faster, gets higher weight

---

## üîß Feature Engineering

### **Feature Groups (Total: ~50 features)**

#### **1. Rolling Statistics (21 sensors √ó 3 windows = 63 ‚Üí top 20)**
```python
windows = [10, 25, 50]  # cycles
aggregations = ['mean', 'std', 'max']

# Example for sensor_2 (temperature)
features = [
    'sensor_2_mean_10',   # short-term trend
    'sensor_2_std_25',    # medium-term volatility
    'sensor_2_max_50'     # long-term peak
]
```

**Selection:** Keep top 20 by XGBoost importance

#### **2. Lag Features (sensors = 10)**
```python
# Most degradation-sensitive sensors
critical_sensors = [2, 3, 4, 7, 11, 12, 13, 15, 17, 21]

for sensor in critical_sensors:
    features.append(f'sensor_{sensor}_lag_1')   # previous cycle
    features.append(f'sensor_{sensor}_lag_10')  # 10 cycles ago
```

**Output:** 10 sensors √ó 1 lag = 10 features

#### **3. Rate of Change (10 features)**
```python
for sensor in critical_sensors:
    # Degradation velocity
    features.append(f'sensor_{sensor}_roc_10')  # change over 10 cycles
```

#### **4. Domain Features (10 features)**
```python
# Temperature stress indicator
'temp_stress': (sensor_2 + sensor_3 + sensor_4) / 3

# Vibration anomaly
'vibration_anomaly': sensor_11 > sensor_11_mean_50 + 2*std

# Cycles since anomaly
'cycles_since_anomaly': count cycles since vibration spike

# Operating regime (one-hot encoded)
'regime_low', 'regime_medium', 'regime_high'

# Degradation score (composite)
'degradation_score': weighted sum of sensor deviations
```

**Total Features:** 20 + 10 + 10 + 10 = **50 features**

### **Feature Store Design**

**Storage:** Redis (in-memory for <5ms access)

```python
# Key structure
feature_key = f"features:equipment_{id}:timestamp_{ts}"

# Value: JSON with features + metadata
{
    "features": [...],  # 50-element array
    "computed_at": "2026-02-07T10:00:00Z",
    "version": "v1.2"
}
```

**TTL:** 90 days (training needs historical features)
**Backup:** PostgreSQL (persistent storage)

---

## üèóÔ∏è System Architecture

### **Service Stack**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CLIENT APPLICATIONS                      ‚îÇ
‚îÇ         (Maintenance Dashboard, API Consumers)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                    [HTTPS/REST]
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   PREDICTION API                             ‚îÇ
‚îÇ         FastAPI + Uvicorn (3 replicas)                       ‚îÇ
‚îÇ    /predict  /batch-predict  /explain  /health              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                                             ‚îÇ
    ‚îÇ (features)                            (log predictions)
    ‚ñº                                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     REDIS     ‚îÇ                          ‚îÇ PostgreSQL   ‚îÇ
‚îÇ Feature Store ‚îÇ                          ‚îÇ  Predictions ‚îÇ
‚îÇ  (< 5ms read) ‚îÇ                          ‚îÇ   + Actuals  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤                                        ‚îÇ
         ‚îÇ                                        ‚îÇ
         ‚îÇ (write features)              (read training data)
         ‚îÇ                                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  TRAINING PIPELINE                         ‚îÇ
‚îÇ    (Prefect DAG, runs weekly + on-demand)                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  [Data Prep] ‚Üí [Feature Eng] ‚Üí [Train] ‚Üí [Evaluate]       ‚îÇ
‚îÇ        ‚Üì            ‚Üì             ‚Üì          ‚Üì             ‚îÇ
‚îÇ    [Validate]   [Store]      [MLflow]   [Register]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                    [artifacts]
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   MLFLOW SERVER                              ‚îÇ
‚îÇ     Experiment Tracking + Model Registry                     ‚îÇ
‚îÇ     Backend: PostgreSQL  |  Artifacts: Local filesystem      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MONITORING STACK                                ‚îÇ
‚îÇ   Prometheus ‚Üí Grafana ‚Üí Alertmanager ‚Üí Slack               ‚îÇ
‚îÇ  (metrics)    (dashboards)  (alerts)    (notifications)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Docker Compose Services**

```yaml
services:
  # Core Data Layer
  postgres:
    image: postgres:15-alpine
    volumes: pgdata
    ports: 5432
    resources: 2 CPU, 4GB RAM
  
  redis:
    image: redis:7-alpine
    ports: 6379
    resources: 1 CPU, 2GB RAM
  
  # MLOps Layer
  mlflow:
    build: ./mlflow
    ports: 5000
    depends_on: postgres
    resources: 1 CPU, 2GB RAM
  
  # Application Layer
  api:
    build: ./src/api
    ports: 8000
    replicas: 3
    depends_on: [postgres, redis, mlflow]
    resources: 2 CPU, 4GB RAM per replica
    environment:
      - CUDA_VISIBLE_DEVICES=-1  # CPU inference (fast enough)
  
  # Monitoring Layer
  prometheus:
    image: prom/prometheus:latest
    ports: 9090
    volumes: ./prometheus.yml
    resources: 1 CPU, 2GB RAM
  
  grafana:
    image: grafana/grafana:latest
    ports: 3000
    depends_on: prometheus
    resources: 1 CPU, 1GB RAM
  
  # Training (on-demand, not always running)
  trainer:
    build: ./src/training
    depends_on: [postgres, redis, mlflow]
    deploy: manual
    environment:
      - CUDA_VISIBLE_DEVICES=0  # Use GPU
    resources: 4 CPU, 8GB RAM, 1 GPU
```

**Total Resource Usage:**
- Idle: ~12GB RAM, minimal CPU
- Training: +8GB RAM, GPU active

---

## üîÑ MLOps Pipeline

### **Pipeline Orchestration: Prefect**

**Why Prefect over Airflow?**
- Lighter weight (no separate webserver)
- Better for personal projects
- Native Python API
- Easier debugging

### **Pipeline DAGs**

#### **1. Data Ingestion Pipeline**
**Trigger:** Manual (hourly in production)
**Duration:** ~2 minutes

```python
@flow
def data_ingestion_pipeline():
    # 1. Extract from source (simulated IoT)
    raw_data = extract_sensor_data()
    
    # 2. Validate with Great Expectations
    validation_results = validate_data(raw_data)
    
    if not validation_results.success:
        send_alert("Data validation failed")
        return
    
    # 3. Store in PostgreSQL
    store_raw_data(raw_data)
    
    # 4. Trigger feature pipeline
    feature_engineering_pipeline()
```

#### **2. Feature Engineering Pipeline**
**Trigger:** After data ingestion
**Duration:** ~3 minutes for 100 equipment

```python
@flow
def feature_engineering_pipeline():
    # 1. Load recent data (last 100 cycles per equipment)
    data = load_data_for_features()
    
    # 2. Compute features
    features = compute_features(data)
    
    # 3. Validate feature distributions
    validate_features(features)
    
    # 4. Store in Redis + PostgreSQL
    store_features(features)
```

#### **3. Training Pipeline**
**Trigger:** Weekly (Sunday 2 AM) OR on-demand OR drift detected
**Duration:** ~15 minutes

```python
@flow
def training_pipeline(trigger_reason: str):
    with mlflow.start_run():
        # 1. Load training data (last 60 days)
        X_train, y_train = load_training_data()
        X_val, y_val = load_validation_data()
        
        # 2. Train XGBoost
        xgb_model = train_xgboost(X_train, y_train)
        xgb_metrics = evaluate_model(xgb_model, X_val, y_val)
        
        # 3. Train LSTM (GPU)
        lstm_model = train_lstm(X_train, y_train, device='cuda')
        lstm_metrics = evaluate_model(lstm_model, X_val, y_val)
        
        # 4. Create ensemble
        ensemble = create_ensemble(xgb_model, lstm_model)
        ensemble_metrics = evaluate_model(ensemble, X_val, y_val)
        
        # 5. Log to MLflow
        mlflow.log_params(...)
        mlflow.log_metrics(ensemble_metrics)
        
        # 6. Model validation gate
        if ensemble_metrics['f2_score'] > 0.78:
            # 7. Register model
            model_uri = mlflow.register_model(
                model_uri=f"runs:/{mlflow.active_run().info.run_id}/model",
                name="predictive_maintenance_ensemble"
            )
            
            # 8. Transition to staging
            transition_model_stage(model_uri, stage="Staging")
            
            # 9. Run shadow deployment tests
            shadow_test_results = run_shadow_tests(ensemble)
            
            # 10. Promote to production if tests pass
            if shadow_test_results.pass_rate > 0.95:
                transition_model_stage(model_uri, stage="Production")
                send_alert(f"New model deployed: {model_uri}")
            else:
                send_alert(f"Shadow tests failed, keeping old model")
        else:
            send_alert(f"Model F2 {ensemble_metrics['f2_score']:.3f} < 0.78, not deploying")
```

#### **4. Drift Detection Pipeline**
**Trigger:** Every 6 hours
**Duration:** ~1 minute

```python
@flow
def drift_detection_pipeline():
    # 1. Load reference data (last 30 days)
    reference_data = load_reference_data()
    
    # 2. Load current data (last 6 hours)
    current_data = load_current_data()
    
    # 3. Feature drift (Evidently)
    feature_drift_report = generate_feature_drift_report(
        reference_data, current_data
    )
    
    # 4. Prediction drift (PSI)
    prediction_drift = calculate_psi(
        reference_predictions, current_predictions
    )
    
    # 5. Check thresholds
    if feature_drift_report.get_drift_share() > 0.3:
        send_alert("‚ö†Ô∏è Feature drift detected: 30% of features drifted")
        # Trigger retraining
        training_pipeline.apply_async(trigger_reason="feature_drift")
    
    if prediction_drift > 0.25:
        send_alert("‚ö†Ô∏è Prediction drift detected: PSI = {prediction_drift:.3f}")
        training_pipeline.apply_async(trigger_reason="prediction_drift")
    
    # 6. Store drift metrics
    store_drift_metrics(feature_drift_report, prediction_drift)
```

---

## üöÄ API Design

### **Endpoints**

#### **1. Health Check**
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_version": "v1.3.2",
  "services": {
    "postgres": "ok",
    "redis": "ok",
    "mlflow": "ok"
  },
  "uptime_seconds": 86400
}
```

#### **2. Single Prediction**
```http
POST /predict
Content-Type: application/json

{
  "equipment_id": "engine_042",
  "sensor_readings": {
    "sensor_1": 518.67,
    "sensor_2": 642.83,
    ...
    "sensor_21": 23.42
  },
  "operational_settings": {
    "setting_1": -0.0007,
    "setting_2": -0.0004,
    "setting_3": 100.0
  },
  "timestamp": "2026-02-07T10:00:00Z"
}
```

**Response (< 50ms):**
```json
{
  "equipment_id": "engine_042",
  "prediction": {
    "failure_probability": 0.82,
    "risk_level": "HIGH",
    "confidence": 0.91,
    "predicted_rul_cycles": 15,
    "predicted_rul_days": 3.75,
    "should_inspect": true
  },
  "explanation": {
    "top_features": [
      {
        "feature": "sensor_11_mean_25",
        "importance": 0.23,
        "value": 47.82,
        "impact": "increases_risk"
      },
      {
        "feature": "sensor_2_roc_10",
        "importance": 0.19,
        "value": 15.3,
        "impact": "increases_risk"
      },
      {
        "feature": "degradation_score",
        "importance": 0.15,
        "value": 0.78,
        "impact": "increases_risk"
      }
    ],
    "visualization_url": "/shap/engine_042/latest"
  },
  "model_version": "v1.3.2",
  "timestamp": "2026-02-07T10:00:01Z",
  "latency_ms": 42
}
```

#### **3. Batch Prediction**
```http
POST /batch-predict
Content-Type: application/json

{
  "equipment_ids": ["engine_001", "engine_002", ..., "engine_100"],
  "timestamp": "2026-02-07T10:00:00Z"
}
```

**Response (< 2 seconds for 100 equipment):**
```json
{
  "predictions": [
    { "equipment_id": "engine_001", "failure_probability": 0.15, ... },
    { "equipment_id": "engine_002", "failure_probability": 0.67, ... },
    ...
  ],
  "total_count": 100,
  "high_risk_count": 8,
  "processing_time_ms": 1847
}
```

#### **4. Feedback Submission**
```http
POST /feedback
Content-Type: application/json

{
  "equipment_id": "engine_042",
  "prediction_id": "pred_20260207_100001_042",
  "actual_outcome": {
    "failure_occurred": true,
    "failure_timestamp": "2026-02-10T08:30:00Z",
    "failure_type": "compressor_blade_break",
    "inspection_notes": "Visible crack on blade 7"
  }
}
```

**Response:**
```json
{
  "feedback_id": "fb_20260210_100530_042",
  "status": "recorded",
  "prediction_accuracy": {
    "predicted_rul_days": 3.75,
    "actual_rul_days": 3.15,
    "error_days": 0.60
  },
  "model_updated": false,
  "next_training_scheduled": "2026-02-14T02:00:00Z"
}
```

---

## üìä Monitoring & Observability

### **Metrics to Track**

#### **Model Performance**
```python
# Prometheus metrics
model_prediction_total = Counter('predictions_total', 'Total predictions')
model_prediction_latency = Histogram('prediction_latency_seconds')
model_confidence = Histogram('prediction_confidence')
model_f2_score = Gauge('model_f2_score_current')
model_precision = Gauge('model_precision_current')
model_recall = Gauge('model_recall_current')
model_false_alarm_rate = Gauge('model_false_alarm_rate')
```

#### **Data Quality**
```python
data_missing_rate = Gauge('data_missing_value_rate')
data_outlier_count = Counter('data_outliers_total')
feature_drift_score = Gauge('feature_drift_score', ['feature_name'])
prediction_drift_psi = Gauge('prediction_drift_psi')
```

#### **System Health**
```python
api_requests_total = Counter('api_requests_total', ['endpoint', 'status'])
api_latency = Histogram('api_latency_seconds', ['endpoint'])
api_errors_total = Counter('api_errors_total', ['endpoint', 'error_type'])
db_connection_pool_size = Gauge('db_connections_active')
redis_cache_hit_rate = Gauge('redis_cache_hit_rate')
```

### **Grafana Dashboards**

#### **Dashboard 1: Model Performance**
- F2 / Precision / Recall over time (line chart)
- Confusion matrix (heatmap, updated daily)
- Prediction distribution (histogram)
- Confidence intervals (box plot)
- False alarm rate trend

#### **Dashboard 2: API Health**
- Request rate (requests/second)
- Latency percentiles (p50, p95, p99)
- Error rate (%)
- Success rate by endpoint
- Response time heatmap

#### **Dashboard 3: Data & Drift**
- Feature drift scores (bar chart, top 10)
- Prediction drift PSI (gauge)
- Missing value rate (line chart)
- Data ingestion lag (seconds)
- Training data freshness

#### **Dashboard 4: Business KPIs**
- Prevented failures (count)
- Cost savings estimate ($)
- Equipment under monitoring
- High-risk equipment list (table)
- Monthly maintenance schedule

### **Alerting Rules**

**Critical Alerts (Slack + Email):**
```yaml
- Model F2 score < 0.75 for 24 hours
- API down (no successful requests for 5 minutes)
- Prediction drift PSI > 0.40
- Training pipeline failed 2x in a row
- Database connection pool exhausted
```

**Warning Alerts (Slack only):**
```yaml
- Model F2 score < 0.78
- API p95 latency > 100ms for 10 minutes
- Prediction drift PSI > 0.25
- Feature drift detected in >30% of features
- Cache hit rate < 70%
- Data ingestion delayed > 1 hour
```

---

## üß™ Testing Strategy

### **Test Coverage Target: 85%**

#### **Unit Tests (pytest)**
```
tests/
‚îú‚îÄ‚îÄ test_features.py          # Feature engineering functions
‚îú‚îÄ‚îÄ test_models.py            # Model prediction logic
‚îú‚îÄ‚îÄ test_data_validation.py   # Great Expectations suites
‚îú‚îÄ‚îÄ test_utils.py             # Helper functions
‚îî‚îÄ‚îÄ test_drift_detection.py   # Drift calculation
```

**Example:**
```python
def test_rolling_mean_feature():
    """Test rolling mean calculation for sensor data."""
    data = pd.DataFrame({
        'sensor_2': [10, 20, 30, 40, 50],
        'cycle': [1, 2, 3, 4, 5]
    })
    
    result = compute_rolling_mean(data, window=3, sensor='sensor_2')
    
    assert result.iloc[2] == 20.0  # (10+20+30)/3
    assert result.iloc[4] == 40.0  # (30+40+50)/3

def test_xgboost_prediction_shape():
    """Test XGBoost model output shape."""
    model = load_model('xgboost_v1.pkl')
    X_test = np.random.rand(10, 50)  # 10 samples, 50 features
    
    predictions = model.predict_proba(X_test)
    
    assert predictions.shape == (10, 2)  # 2 classes
    assert np.all((predictions >= 0) & (predictions <= 1))
```

#### **Integration Tests**
```
tests/integration/
‚îú‚îÄ‚îÄ test_api_endpoints.py     # End-to-end API tests
‚îú‚îÄ‚îÄ test_training_pipeline.py # Full training workflow
‚îú‚îÄ‚îÄ test_database.py          # Database operations
‚îî‚îÄ‚îÄ test_feature_store.py     # Redis operations
```

**Example:**
```python
@pytest.mark.integration
def test_predict_endpoint_e2e(test_client):
    """Test full prediction flow through API."""
    payload = {
        "equipment_id": "test_engine",
        "sensor_readings": {...},  # valid sensor data
        "operational_settings": {...}
    }
    
    response = test_client.post("/predict", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert 0 <= data['prediction']['failure_probability'] <= 1
    assert 'explanation' in data
    assert data['latency_ms'] < 50
```

#### **Load Tests (Locust)**
```python
class PredictionUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def predict(self):
        payload = generate_random_sensor_data()
        self.client.post("/predict", json=payload)
    
    @task(3)  # 3x more frequent
    def health_check(self):
        self.client.get("/health")
```

**Load Test Targets:**
- 1000 concurrent users
- 10-minute sustained load
- Success rate > 99.9%
- p95 latency < 50ms

---

## üìÅ Project Structure

```
predictive-maintenance-mlops/
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Data directory (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # NASA Turbofan dataset
‚îÇ   ‚îú‚îÄ‚îÄ processed/                 # Cleaned data
‚îÇ   ‚îú‚îÄ‚îÄ features/                  # Engineered features
‚îÇ   ‚îî‚îÄ‚îÄ models/                    # Trained model artifacts
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb              # Exploratory Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_baseline_xgboost.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_lstm_model.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 05_ensemble.ipynb
‚îÇ
‚îú‚îÄ‚îÄ src/                           # Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Data processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.py             # Data loading utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validator.py          # Great Expectations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py       # Data cleaning
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ features/                  # Feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ builder.py            # Feature computation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store.py              # Redis feature store
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selector.py           # Feature selection
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # ML models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xgboost_model.py      # XGBoost wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm_model.py         # PyTorch LSTM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ensemble.py           # Ensemble logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ explainer.py          # SHAP wrapper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                  # Training pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py            # Training orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py          # Model evaluation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hyperparameter_tuner.py  # Optuna
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ serving/                   # Prediction service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py                # FastAPI app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py          # Prediction logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py            # Pydantic models
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/                # Monitoring & drift
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ drift_detector.py     # Evidently wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py            # Prometheus metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alerting.py           # Slack notifications
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/                 # Orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.py      # Data ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py  # Model training
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring_pipeline.py # Drift detection
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                     # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py             # Configuration management
‚îÇ       ‚îú‚îÄ‚îÄ logger.py             # Structured logging
‚îÇ       ‚îî‚îÄ‚îÄ db.py                 # Database connections
‚îÇ
‚îú‚îÄ‚îÄ tests/                         # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ load/
‚îÇ
‚îú‚îÄ‚îÄ configs/                       # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ training_config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ api_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ monitoring_config.yaml
‚îÇ
‚îú‚îÄ‚îÄ docker/                        # Docker configurations
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.api
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.trainer
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.mlflow
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ deploy/                        # Deployment configs
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards/
‚îÇ   ‚îî‚îÄ‚îÄ prefect/
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ api_documentation.md
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ deployment_guide.md
‚îÇ   ‚îî‚îÄ‚îÄ troubleshooting.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_db.sh
‚îÇ   ‚îú‚îÄ‚îÄ load_data.py
‚îÇ   ‚îú‚îÄ‚îÄ run_training.py
‚îÇ   ‚îî‚îÄ‚îÄ deploy_model.sh
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml                # Run tests on PR
‚îÇ       ‚îî‚îÄ‚îÄ cd.yml                # Deploy on main
‚îÇ
‚îú‚îÄ‚îÄ requirements/                  # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ base.txt                  # Core deps
‚îÇ   ‚îú‚îÄ‚îÄ training.txt              # ML training
‚îÇ   ‚îú‚îÄ‚îÄ serving.txt               # API serving
‚îÇ   ‚îî‚îÄ‚îÄ dev.txt                   # Development tools
‚îÇ
‚îú‚îÄ‚îÄ .env.example                   # Environment variables template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ pyproject.toml                # Project metadata
‚îú‚îÄ‚îÄ pytest.ini                    # Test configuration
‚îú‚îÄ‚îÄ README.md                     # Project overview
‚îî‚îÄ‚îÄ PROJECT_SPECIFICATION.md      # This file
```

---

## üîê Security & Configuration

### **Environment Variables**

```bash
# Database
DATABASE_URL=postgresql://user:password@postgres:5432/predictive_maintenance
REDIS_URL=redis://redis:6379/0

# MLflow
MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_BACKEND_STORE_URI=postgresql://user:password@postgres:5432/mlflow
MLFLOW_ARTIFACT_ROOT=/mlflow/artifacts

# API
API_SECRET_KEY=<generate_with_openssl_rand_hex_32>
API_RATE_LIMIT=100  # requests per minute

# Monitoring
PROMETHEUS_URL=http://prometheus:9090
GRAFANA_ADMIN_PASSWORD=<secure_password>
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# GPU
CUDA_VISIBLE_DEVICES=0  # Use GPU 0 for training
```

### **Security Best Practices**

1. ‚úÖ Never commit `.env` file (use `.env.example`)
2. ‚úÖ Use secrets management (Docker secrets or Vault in production)
3. ‚úÖ Enable HTTPS for API (Traefik or nginx reverse proxy)
4. ‚úÖ Implement rate limiting (100 req/min per API key)
5. ‚úÖ Use JWT tokens with 1-hour expiration
6. ‚úÖ Sanitize all user inputs (Pydantic validation)
7. ‚úÖ Regular dependency updates (Dependabot)

---

## üéØ Success Criteria

### **Technical Metrics**

| Metric | Target | Critical? |
|--------|--------|-----------|
| F2 Score | > 0.80 | ‚úÖ YES |
| Precision | > 0.65 | ‚ö†Ô∏è Nice-to-have |
| Recall | > 0.85 | ‚úÖ YES |
| API Latency (p95) | < 50ms | ‚úÖ YES |
| Training Time | < 15 min | ‚ö†Ô∏è Nice-to-have |
| Test Coverage | > 85% | ‚úÖ YES |
| Drift Detection Working | Yes | ‚úÖ YES |
| Automated Retraining | Yes | ‚úÖ YES |

### **Project Deliverables**

- ‚úÖ **Working Platform**: Fully functional system with Docker Compose
- ‚úÖ **Documentation**: API docs, architecture diagrams, setup guide
- ‚úÖ **Demo Video**: 5-8 minute walkthrough (screen recording)
- ‚úÖ **Blog Post**: Technical deep-dive (Medium/Dev.to)
- ‚úÖ **GitHub Repo**: Clean commit history, README, CI/CD badges

### **Portfolio Impact**

**This project demonstrates:**
1. End-to-end ML system design
2. Production MLOps practices
3. Deep learning with GPU (PyTorch)
4. API development (FastAPI)
5. Containerization (Docker)
6. Monitoring & observability
7. Software engineering best practices

**Target Audience:** ML Engineer, MLOps Engineer, Data Scientist (production) roles

---

## üìÖ Weekly Milestone Checklist

### **Week 1: Setup & EDA**
- [ ] Clone NASA Turbofan dataset
- [ ] Setup Python environment (Python 3.11 + CUDA 13.1)
- [ ] Exploratory data analysis notebook
- [ ] Understand failure patterns
- [ ] Define initial feature list (30 features)

### **Week 2: Feature Engineering**
- [ ] Implement rolling statistics
- [ ] Implement lag features
- [ ] Implement rate of change features
- [ ] Implement domain features
- [ ] Feature validation tests

### **Week 3: Baseline Model**
- [ ] Train XGBoost baseline
- [ ] Achieve F2 > 0.75
- [ ] Setup MLflow tracking
- [ ] Log experiments
- [ ] Feature importance analysis

### **Week 4: LSTM Model**
- [ ] Implement LSTM architecture (PyTorch)
- [ ] Train on GPU (RTX 5060)
- [ ] Sequence preparation pipeline
- [ ] Model evaluation (F2 > 0.74)
- [ ] GPU utilization monitoring

### **Week 5: Ensemble & API**
- [ ] Implement weighted ensemble
- [ ] Calibrate probabilities
- [ ] Ensemble evaluation (F2 > 0.80)
- [ ] FastAPI project setup
- [ ] Implement /predict endpoint

### **Week 6: API Completion**
- [ ] Implement /batch-predict
- [ ] Implement /health
- [ ] Implement /feedback
- [ ] SHAP integration
- [ ] API tests (unit + integration)

### **Week 7: Docker & Database**
- [ ] PostgreSQL schema design
- [ ] Redis feature store
- [ ] Dockerfiles (api, trainer, mlflow)
- [ ] Docker Compose configuration
- [ ] Local deployment test

### **Week 8: Training Pipeline**
- [ ] Prefect setup
- [ ] Data ingestion DAG
- [ ] Feature engineering DAG
- [ ] Training DAG
- [ ] Pipeline tests

### **Week 9: MLOps Features**
- [ ] Model registry (MLflow)
- [ ] Model versioning
- [ ] Automated retraining logic
- [ ] Shadow deployment
- [ ] Rollback mechanism

### **Week 10: Monitoring**
- [ ] Prometheus metrics
- [ ] Drift detection (Evidently)
- [ ] Grafana dashboards
- [ ] Alerting (Slack webhook)
- [ ] Load testing (Locust)

### **Week 11: Polish & Documentation**
- [ ] Complete README.md
- [ ] API documentation (Swagger)
- [ ] Architecture diagrams
- [ ] Deployment guide
- [ ] Code cleanup & refactoring

### **Week 12: Demo & Release**
- [ ] Record demo video
- [ ] Write technical blog post
- [ ] GitHub release (v1.0.0)
- [ ] LinkedIn/Twitter post
- [ ] Portfolio website update

---

## üõ†Ô∏è Technology Stack Summary

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **Language** | Python | 3.11 | Core development |
| **ML Framework** | PyTorch | 2.2+ | LSTM model (GPU) |
| **Tree Models** | XGBoost | 2.0+ | Gradient boosting (GPU) |
| **API** | FastAPI | 0.110+ | REST API |
| **Orchestration** | Prefect | 2.14+ | Pipeline automation |
| **Tracking** | MLflow | 2.10+ | Experiment tracking |
| **Database** | PostgreSQL | 15 | Data lake |
| **Cache** | Redis | 7 | Feature store |
| **Monitoring** | Prometheus | 2.50+ | Metrics collection |
| **Visualization** | Grafana | 10.3+ | Dashboards |
| **Drift** | Evidently AI | 0.4+ | Data/prediction drift |
| **Validation** | Great Expectations | 0.18+ | Data quality |
| **Testing** | pytest | 8.0+ | Unit/integration tests |
| **Load Testing** | Locust | 2.20+ | Performance testing |
| **Containerization** | Docker | 25+ | Deployment |
| **Explainability** | SHAP | 0.44+ | Model interpretation |
| **Data Science** | pandas, NumPy, scikit-learn | Latest | Data processing |

---

## üö® Risk Mitigation

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|---------------------|
| **Model underperforms (F2 < 0.80)** | Low | High | Start with proven architecture (XGBoost + LSTM); extensive feature engineering; have baseline |
| **GPU memory overflow (>8GB)** | Low | Medium | Batch size tuning; gradient accumulation; model pruning |
| **Training takes too long (>15min)** | Medium | Low | Use GPU; cache intermediate results; profile code |
| **Docker resource constraints** | Low | Medium | Resource limits in compose; monitor with cAdvisor |
| **Time overrun** | Medium | Medium | Prioritize Phase 1-2; defer monitoring to Phase 4 |
| **Dataset issues** | Low | High | Multiple failure modes available; can use FD002-FD004 |
| **Integration complexity** | Low | Medium | Incremental integration; frequent testing |

---

## üìà Next Steps (Start Here!)

### **Immediate Actions:**

1. **Create Project Structure** (10 minutes)
   ```bash
   mkdir -p {data/{raw,processed,features,models},notebooks,src/{data,features,models,training,serving,monitoring,pipelines,utils},tests/{unit,integration,load},configs,docker,deploy,docs,scripts}
   ```

2. **Setup Python Environment** (15 minutes)
   ```bash
   conda create -n pred-maint python=3.11
   conda activate pred-maint
   conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
   pip install xgboost[gpu] pandas numpy scikit-learn mlflow fastapi
   ```

3. **Load Dataset** (5 minutes)
   - Data already in `turbofan_ed_dataset/`
   - Read `readme.txt`
   - Start with `train_FD001.txt` and `test_FD001.txt`

4. **Create First Notebook** (30 minutes)
   - `notebooks/01_eda.ipynb`
   - Load data, explore sensors, visualize degradation

---

## üí° Key Decisions Made

1. **GPU Utilization**: Training only (not inference) - API uses CPU for simplicity
2. **Orchestration**: Prefect over Airflow - lighter for personal project
3. **Feature Count**: 50 (not 400) - avoid overfitting, faster iteration
4. **Model Weights**: XGBoost 60%, LSTM 40% - XGBoost more reliable
5. **Monitoring Delay**: Phase 4 - prioritize working model first
6. **Deployment**: Docker Compose - simpler than Kubernetes for portfolio
7. **Dataset**: FD001 primary - simplest failure mode for MVP

---

**Document Version:** 1.0  
**Last Updated:** February 7, 2026  
**Author:** [Your Name]  
**Status:** Ready to Start üöÄ
